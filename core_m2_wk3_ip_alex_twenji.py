# -*- coding: utf-8 -*-
"""Core_M2_Wk3_IP_Alex_Twenji.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rk_9pGY0gTUbcJY1telMMjBGkdksYSXO

# DEFINING THE QUESTION

## a) Specifying the Question

Nairobi Hospital conducted a clinical camp to test for hypothyroidism. The data collected focused on Thyroid patients. Use the healthcare datasets provided to accomplish the following:  

Build a model that determines whether or not the patient's symptoms indicate that the patient has hypothyroid.

---

## b) Defining the Metric for Success

Being able to predict if a patient has hypothyroid

---

## c) Understanding the Context

Hypothyroidism is a condition in which the thyroid gland doesn't produce enough thyroid hormone. This deficiency of thyroid hormones can disrupt such things as heart rate, body temperature and all aspects of metabolism. Hypothyroidism is most prevalent in older women. Major symptoms include fatigue, cold sensitivity, constipation, dry skin and unexplained weight gain. However, Hypothyroidism may not cause noticeable symptoms in the early stages.
Treatment consists of thyroid hormone replacement.

---

## d) Experimental Design

1. Read and explore the given dataset.
2. Find and deal with outliers, anomalies, and missing data within the dataset.
3. Perform univariate and bivariate analysis recording your observations.
4. Perform Exploratory Data Analysis.
5. Performing Analysis using Desicion Trees.
6. Performing Analysis using Support Vector Machines (SVM).
6. Provide a recommendation based on the analysis. 
7. Challenge the solution by providing insights on how improvements can be made.  

---

# DATA PREPARATION
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as sp

# %matplotlib inline

thyroid = pd.read_csv('/content/hypothyroid.csv')
thyroid.head()

thyroid.shape

thyroid.info()

"""# DATA CLEANING"""

thyroid.isna().value_counts()
#no null values.

thyroid.duplicated().value_counts()

# This shows that there are possibbly 77 duplicated records, let's view them.

thyroid[thyroid.duplicated()].head(50)

# The records show that even though some symptoms are duplicated, the ages and the other symptoms
# indicate they are different records.

thyroid.describe()

# Since all columns are objects, we need to convert them for analysis, but before that, we need to deal with the ?s

thyroid.age.replace(to_replace='?', value='0',inplace= True)
thyroid.age = thyroid.age.astype(int)

age_mean = thyroid[thyroid.age != 0]
age_mean = age_mean.age.mean()
age_mean

thyroid.age.replace(to_replace= 0, value = 51, inplace= True)

# Now we've fixed the age column, since the ? were missing values that did not show
# when we considered the column as an object.

# Lets do the same to the other columns that should be numerical, i.e. TSH, T3, TT4, T4U, FTI and TBG

thyroid.TSH.replace(to_replace='?', value='0',inplace= True)
thyroid.T3.replace(to_replace='?', value='0',inplace= True)
thyroid.TT4.replace(to_replace='?', value='0',inplace= True)
thyroid.T4U.replace(to_replace='?', value='0',inplace= True)
thyroid.FTI.replace(to_replace='?', value='0',inplace= True)
thyroid.TBG.replace(to_replace='?', value='0',inplace= True)

thyroid.TSH = thyroid.TSH.astype(float)
thyroid.T3 = thyroid.T3.astype(float)
thyroid.TT4 = thyroid.TT4.astype(float)
thyroid.T4U = thyroid.T4U.astype(float)
thyroid.FTI = thyroid.FTI.astype(float)
thyroid.TBG = thyroid.TBG.astype(float)

TSH_mean = thyroid[thyroid.TSH != 0]
TSH_mean = TSH_mean.TSH.mean()
T3_mean = thyroid[thyroid.T3 != 0]
T3_mean = T3_mean.T3.mean()
TT4_mean = thyroid[thyroid.TT4 != 0]
TT4_mean = TT4_mean.TT4.mean()
T4U_mean = thyroid[thyroid.T4U != 0]
T4U_mean = T4U_mean.T4U.mean()
FTI_mean = thyroid[thyroid.FTI != 0]
FTI_mean = FTI_mean.FTI.mean()
TBG_mean = thyroid[thyroid.TBG != 0]
TBG_mean = TBG_mean.TBG.mean()

thyroid.TSH.replace(to_replace= 0, value = TSH_mean, inplace= True)
thyroid.T3.replace(to_replace= 0, value = T3_mean, inplace= True)
thyroid.TT4.replace(to_replace= 0, value = TT4_mean, inplace= True)
thyroid.T4U.replace(to_replace= 0, value = T4U_mean, inplace= True)
thyroid.FTI.replace(to_replace= 0, value = FTI_mean, inplace= True)
thyroid.TBG.replace(to_replace= 0, value = TBG_mean, inplace= True)

# let's deal with the symptoms columns that mostly have 2 variables.
# replace with u for unknown

thyroid.replace(to_replace='?', value='u', inplace= True)

thyroid.info()

thyroid.describe()

thyroid.describe(include='all')

"""The Dataset is relatively clean now.

# DATA ANALYSIS

## UNIVARIATE ANALYSIS
"""

col_names = ['age',	'TSH',	'T3',	'TT4',	'T4U',	'FTI',	'TBG']

fig, ax = plt.subplots(len(col_names), figsize= (8,40))

for i, col_val in enumerate(col_names):
  sns.boxplot(y = thyroid[col_val], ax= ax[i])
  ax[i].set_title('Box plot - {}'.format(col_val), fontsize= 10)
  ax[i].set_xlabel(col_val, fontsize= 8)
plt.show()

# Apart from Age all the other numerical columns contain outliers.
Quantile_1 = thyroid.quantile(.25)
Quantile_3 = thyroid.quantile(.75)
IQR_values = Quantile_3 - Quantile_1

anomalies = ((thyroid < Quantile_1 - 1.5* IQR_values) | (thyroid > Quantile_3 + 1.5 * IQR_values)).sum()
anomalies

percent_anomalies = (anomalies.sum() / thyroid.shape[0])*100
percent_anomalies

"""The outliers seem like reasonable data that cannot be removed as this would affect the analysis, since the rows involved are roughly 47% of the data.

### Summary Statistics
"""

thyroid.describe()

# Central Tendancies

# mean
for i, col_val in enumerate(col_names):
  print('The mean of ' + str(col_val) + ' is ' + str(thyroid[col_val].mean()))

# median

for i, col_val in enumerate(col_names):
  print('The median of ' + str(col_val) + ' is ' + str(thyroid[col_val].median()))

# the medians are similar to the means and could be due to the cleaning we did to get
# rid of the '?'s

# mode

for i, col_val in enumerate(col_names):
  print('The mode of ' + str(col_val) + ' is ' + str(thyroid[col_val].mode()))

# The modes are unimodal showing that the data was gathered from the same population.

# range

for i, col_val in enumerate(col_names):
  print('The range of ' + str(col_val) + ' is ' + str(thyroid[col_val].max()-thyroid[col_val].min()))

# standard deviation

for i, col_val in enumerate(col_names):
  print('The standard deviation of ' + str(col_val) + ' is ' + str(thyroid[col_val].std()))
  
# variables with higher range showcase higher standard deviation from the mean.

# variance

for i, col_val in enumerate(col_names):
  print('The variance of ' + str(col_val) + ' is ' + str(thyroid[col_val].var()))

# As expected, variables with higher standard deviation, also have higher variance.

# skewness

for i, col_val in enumerate(col_names):
  print('The skewness of ' + str(col_val) + ' is ' + str(thyroid[col_val].skew()))

# only age has negative skewness

# kurtosis

for i, col_val in enumerate(col_names):
  print('The kurtosis of ' + str(col_val) + ' is ' + str(thyroid[col_val].kurt()))

# only age has negative kurtosis

"""### Univariate Analysis Recommendation

Most of the skewness of the data is positive, indicating a mostly positively skewed dataset on most variables. This means that most distributions have longer tails to the right i.e. right-skewed / leptokurtic. Same goes for kurtosis as The data has mostly positive kurtosis indicating that most variables distributions have heavier tails and taller peaks than the normal distribution.

## BIVARIATE ANALYSIS
"""

sns.pairplot(thyroid)

plt.figure(figsize=(20,10))
sns.heatmap(thyroid.corr(), annot= True)

"""### Bivariate Analysis Recommendation

We can see that there's mostly weak correlation between these variables, however there are some that have moderate correlation:
-> T3 and TT4 (0.54)
-> TT4 and FTI (0.68)

## EXPLORATORY DATA ANALYSIS
"""

# Since from context hypothyrosis is mostly found in women of older age, lets see some distributions.

thyroid.age.hist(bins=10)

hypo_age = thyroid[thyroid.status != 'negative']
hypo_age.age.hist(bins=10)

# as expected, the more elderly people with hypothyrosis are more than the younger ones.

sex_age = hypo_age[hypo_age.sex == 'F']
sex_age.age.hist(bins=10)

# as expected, the more elderly women with hypothyrosis are more than the younger ones.

thyroid.sex.value_counts()

# as expected, there's more women than men with hypothyrosis. 'u' stands for unknown gender.

"""# MODELING

## a) DECISION TREES
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC,LinearSVC
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix
from sklearn.tree import export_graphviz
from sklearn.metrics import  accuracy_score
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus
from sklearn.model_selection import train_test_split
from sklearn import metrics

thyroid.describe(include='all')

thyroid.head()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

thyroid.status = le.fit_transform(thyroid.status)
thyroid.sex = le.fit_transform(thyroid.sex)
thyroid.on_thyroxine = le.fit_transform(thyroid.on_thyroxine)
thyroid.query_on_thyroxine = le.fit_transform(thyroid.query_on_thyroxine)
thyroid.on_antithyroid_medication = le.fit_transform(thyroid.on_antithyroid_medication)
thyroid.thyroid_surgery = le.fit_transform(thyroid.thyroid_surgery)
thyroid.query_hypothyroid = le.fit_transform(thyroid.query_hypothyroid)
thyroid.query_hyperthyroid = le.fit_transform(thyroid.query_hyperthyroid)
thyroid.pregnant = le.fit_transform(thyroid.pregnant)
thyroid.sick = le.fit_transform(thyroid.sick)
thyroid.tumor = le.fit_transform(thyroid.tumor)
thyroid.lithium = le.fit_transform(thyroid.lithium)
thyroid.goitre = le.fit_transform(thyroid.goitre)
thyroid.TSH_measured = le.fit_transform(thyroid.TSH_measured)
thyroid.T3_measured = le.fit_transform(thyroid.T3_measured)
thyroid.TT4_measured = le.fit_transform(thyroid.TT4_measured)
thyroid.T4U_measured = le.fit_transform(thyroid.T4U_measured)
thyroid.FTI_measured = le.fit_transform(thyroid.FTI_measured)
thyroid.TBG_measured = le.fit_transform(thyroid.TBG_measured)

thyroid.describe()

#Preparing Global Variables

X_ = thyroid.drop(columns = 'status')
y_ = thyroid.status

X = X_.values
y = y_.values

testfeatures = X_.columns

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# Create a decision tree classifier

tree = DecisionTreeClassifier()

# Train it on our training set.
tree = tree.fit(X_train, y_train)

# Predict based on the model we've trained
y_pred = tree.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# This will be our baseline accuracy comparison

"""### 1. Random Forest"""

forest = RandomForestRegressor()
forest = forest.fit(X_train, y_train)

# Predict based on the model we've trained
y_pred_forest = forest.predict(X_test)


comparison_frame = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred_forest.flatten()})

comparison_frame.describe()

# And now we assess the errors
# print("Accuracy:",metrics.accuracy_score(y_test, y_pred_forest))
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_forest))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_forest))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_forest)))

# Let's see what tree #5 looks like:

dot_data = StringIO()

tree = forest.estimators_[5]

export_graphviz(tree, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = testfeatures, class_names=['hypothyroid', 'negative'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('baseline_forest.png')
Image(graph.create_png())

# Notice that some of the features exhibited here were never included in our past model. 
# We'll often see these features if you display other trees, but the trees will have slight differences of features or tests in multiple nodes.

# Visualizing feature importance

importances = list(tree.feature_importances_)

# List of tuples with variable and importance
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(testfeatures, importances)]

# We can sort the values in descending order, since we care about the most important features first.
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)

# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

# Plotting the above

# list of x locations for plotting
x_values = list(range(len(importances)))
# Make a bar chart
plt.bar(x_values, importances, orientation = 'vertical')
# Tick labels for x axis
plt.xticks(x_values, testfeatures, rotation='vertical')
# Axis labels and title
plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances')

"""From this, we can see the important features are FTI, TSH, TT4, T4U and T3 which are tests for hormones
that will tell if a person has hypothyroid. Age is also a major factor as was seen in the Exploratory Data Analysis section.

---

### Tuning the Random Forest
"""

# Tuning our Forest with the most important features and max_depth.

y_Forest = y.copy()
X_Forest_ = thyroid[['FTI', 'TSH', 'TT4', 'age', 'T4U', 'T3', 'thyroid_surgery']]
X_Forest = X_Forest_.values

X_train, X_test, y_train, y_test = train_test_split(X_Forest, y_Forest, test_size=0.3, random_state=0)


Forest = RandomForestRegressor(max_depth=5)
Forest = Forest.fit(X_train, y_train)

y_pred_Forest = Forest.predict(X_test)

comparison_frame = pd.DataFrame({'Actual': y_test.flatten(), 'Predicted': y_pred_Forest.flatten()})

comparison_frame.describe()

# And now we assess the errors
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_Forest))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_Forest))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_Forest)))

dot_data = StringIO()
# pick a specific tree from the forest
Tree = Forest.estimators_[5]

export_graphviz(Tree, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = X_Forest_.columns, class_names=['hypothyroid', 'negative'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('tuned_forest.png')
Image(graph.create_png())

# Visualizing the new feature importance

importances_Forest = list(Tree.feature_importances_)

# List of tuples with variable and importance
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X_Forest_.columns, importances_Forest)]

# We can sort the values in descending order, since we care about the most important features first.
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)

# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

# Plotting the above

# list of x locations for plotting
x_values_Forest = list(range(len(importances_Forest)))

plt.bar(x_values_Forest, importances_Forest, orientation = 'vertical')
plt.xticks(x_values_Forest, X_Forest_.columns, rotation='vertical')
plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances')

# We've reduced the important features to just 6, that we can use for prediction.

"""The MAEs of the Random Forests are worse than the Decision Trees, Howeever both their MSEs and RMSE scores are better. Upon tuning, the MSE and RMSE slightly improve on thr Random Forest Model achivieng the best scores so far.

### 2. Gradient Boost
"""

X_grad = X_.values
y_grad = y_.values

testfeatures = X_.columns

X_train, X_test, y_train, y_test = train_test_split(X_grad, y_grad, test_size=0.3, random_state=0)

# We set different learning rates, so that we can compare the performance of the classifier's 
# performance at different learning rates.

learning_rate = [0.001,0.01,0.1,0.5,0.75,1]

for lr in learning_rate:
  grad = GradientBoostingClassifier(learning_rate=lr)
  grad.fit(X_train, y_train)

  print('For Learning rate: ', lr)
  print('Training Accuracy score is: ', grad.score(X_train, y_train))
  print('Validation Accuracy score is: ', grad.score(X_test, y_test))

"""### Tuning Gradient Boost"""

# Validation Accuracy is of our main importance, but since Learning rate 0.5 gives us the best 
# Training Accuracy and a very good Validation Accuracy, we'll use this learning accuracy.

# Let's create a new classifier and specify the best learning rate we discovered.

grad_best = GradientBoostingClassifier(learning_rate= 0.5)
grad_best.fit(X_train, y_train)

grad_pred = grad_best.predict(X_test)

# Checking its accuracy using a confusion matrix

print(confusion_matrix(y_test, grad_pred))

# This is a very accurate model since 33+899 predictions were correct, which is over 98%.

print("Accuracy:",metrics.accuracy_score(y_test, grad_pred))
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, grad_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, grad_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, grad_pred)))

"""The MAE is better than the Random Forest Model, however, the MSE and RMSE are worse.

### Decision Trees Recommendation

The accuracies of the models are all above 98%, however, using the MSE values which would help in precision when predicting value, in order to see if the data was overfitted, the tuned Random Forest is our choice, since it has the lowest MSE. The lower the MSE value the better and 0 means the model is perfect.

## b) SUPPORT VECTOR MACHINES
"""

# For this section we'll need to find out the important features prior to applying our svm models

thyroid.head()

# Checking for Multicolinearity

thyroid_correlation = thyroid.corr()

pd.DataFrame(np.linalg.inv(thyroid_correlation.values), index = thyroid_correlation.index, 
             columns=thyroid_correlation.columns)

# TT4_measured , T4U_measured, FTI_measured, TBG_measured and TT4 all have VIFs way above 5,
# Let's remove them

thyroid_new = thyroid.drop(columns=['TT4_measured', 'T4U_measured', 'FTI_measured', 'TBG_measured', 'TT4'])
new_thyroid_corr = thyroid_new.corr()

pd.DataFrame(np.linalg.inv(new_thyroid_corr.values), index = new_thyroid_corr.index, 
             columns=new_thyroid_corr.columns)

thyroid_new.corr()

 
# FTI from Decision trees indicated the most relevance, we can pick those two from the above list.

thyroid_new.status = le.inverse_transform(thyroid_new.status)
thyroid_new.head()

thyroid_new.status.unique()

thyroid_new.status = thyroid_new.status.replace(to_replace=['n','y'], value = ['hypothyroid','negative'])
thyroid_new.head()

"""### Linear Function Model"""

sns.lmplot('FTI', 'TSH', data=thyroid_new, hue='status',
           palette='Set1', fit_reg=False, scatter_kws={"s": 100});

# Specify the input for the model

hypo = thyroid_new[['FTI', 'TSH']].to_numpy()

type_label = np.where(thyroid_new['status']=='hypothyroid',1,0)


# Fit the model
model = SVC(kernel= 'linear')
model.fit(hypo,type_label)

# Get the separating hyperplane
w = model.coef_[0] #get the first coefficient of our model
a = -w[0] / w[1]
# Get the x values of our hyperplane. We achieve this by creating a range numbers 
# from the largest number of the FTI vlaues and the smallest number of the FTI values.  

xx = np.linspace(0, 400)
yy = a * xx - (model.intercept_[0]) / w[1]

# Plot the hyperplane
sns.lmplot('FTI', 'TSH', data=thyroid_new, hue='status', palette='Set1', fit_reg=False, scatter_kws={"s": 100})
plt.plot(xx, yy, linewidth=2, color='black')

# Splitting the data to see if the model will work.

X_svm = thyroid_new[['TSH', 'FTI']]
y_svm = np.where(thyroid_new['status'] == 'hypothyroid', 1,0)

X_train, X_test, y_train, y_test = train_test_split(X_svm, y_svm, test_size=0.3, random_state=0)

model_lin = SVC(kernel= 'linear')

model_lin.fit(X_train,y_train)

y_pred_lin = model_lin.predict(X_test)

model_lin_accuracy = accuracy_score(y_test, y_pred_lin)
print('Accuracy: ', model_lin_accuracy)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_lin))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_lin))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_lin)))

confusion_matrix(y_test, y_pred_lin)

# This will be our baseline model to compare with.

"""### Polynomial Function Model"""

poly = SVC(kernel= 'poly', degree=3)
poly.fit(X_train, y_train)

y_pred_poly = poly.predict(X_test)

model_poly_accuracy = accuracy_score(y_test, y_pred_poly)
print('Accuracy: ', model_poly_accuracy)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_poly))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_poly))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_poly)))

confusion_matrix(y_test, y_pred_poly)

# all the values i.e. Accuracy, MAE, MSE and RMSE are worse than the linear model's

"""### Radial Basis Function (rbf) Model"""

rbf = SVC(kernel= 'rbf')

rbf.fit(X_train, y_train)

y_pred_rbf = rbf.predict(X_test)

model_rbf_accuracy = accuracy_score(y_test, y_pred_rbf)
print('Accuracy: ', model_rbf_accuracy)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_rbf))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_rbf))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_rbf)))

confusion_matrix(y_test, y_pred_rbf)

# This is the best performing svm model.

"""Since rbf performed the best out of the 3 kernels, we'll select it for tuning

### Tuning rbf Model
"""

Rbf = SVC(kernel= 'rbf', gamma='auto')

Rbf.fit(X_train, y_train)

y_pred_Rbf = Rbf.predict(X_test)

model_Rbf_accuracy = accuracy_score(y_test, y_pred_Rbf)
print('Accuracy: ', model_Rbf_accuracy)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_Rbf))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_Rbf))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_Rbf)))

confusion_matrix(y_test, y_pred_Rbf)

"""### SVM Recommendation

After tuning with gamma, the results worsened, therefore we will stick with the best performing svm rbf model that we had initially developped.

## c) SVM WITH MORE FEATURES
"""

X_Svm = thyroid_new.drop(columns=['status'])
y_Svm = np.where(thyroid_new['status'] == 'hypothyroid', 1,0)

X_train, X_test, y_train, y_test = train_test_split(X_Svm, y_Svm, test_size=0.3, random_state=0)

RBF = SVC(kernel= 'rbf')

RBF.fit(X_train, y_train)

y_pred_RBF = RBF.predict(X_test)

model_RBF_accuracy = accuracy_score(y_test, y_pred_RBF)
print('Accuracy: ', model_RBF_accuracy)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_RBF))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_RBF))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_RBF)))

confusion_matrix(y_test, y_pred_RBF)

"""The results are similar to those of just 2 features

# CONCLUSION

The best performing model from the Decision Trees was the tuned Random Forest Classifier while the best SVM was the untuned rbf kernel model. However, the tuned Random Forest Classifier performed the best and should be considered appropriate for this data.

# CHALLENGING THE SOLUTION
"""

# Since Sigmoid is best used for binary classifications i.e. 0 and 1, and we have
# 2 classes to predict, let's see if the results are any better.

X_Sig = thyroid_new.drop(columns=['status'])
y_Sig = np.where(thyroid_new['status'] == 'hypothyroid', 1,0)

X_train, X_test, y_train, y_test = train_test_split(X_Sig, y_Sig, test_size=0.3, random_state=0)

Sig = SVC(kernel= 'sigmoid')

Sig.fit(X_train, y_train)

y_pred_Sig = Sig.predict(X_test)

model_Sig_accuracy = accuracy_score(y_test, y_pred_Sig)
print('Accuracy: ', model_Sig_accuracy)
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_Sig))  
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_Sig))

confusion_matrix(y_test, y_pred_Sig)

"""This is one of the worst models and ensures that the Random Forest Model we had developped was the best option.

# RECOMMENDATION

The Random Forest Classifier performed the best, however, the svm models can be improved by performing PCA to determine fewer components to use as features to the model. This could improve the prediction results and could be the next step of the project.

---

# FOLLOW UP QUESTIONS

## a) Did we have the right data?

Yes, as our accuracy scores were all above 95%

## b) Do we need other data to answer our question?

Yes, since we had too many questionmarks that we had to fill in using various methods appropriate to each column

## c) Did we have the right question?

Yes we did since the deficiency of thyroid hormones can disrupt such things as heart rate, body temperature and all aspects of metabolism and Hypothyroidism may not cause noticeable symptoms in the early stages
"""